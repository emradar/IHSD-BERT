{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 3389,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014753614635585718,
      "grad_norm": 35.95878601074219,
      "learning_rate": 1.990754401495033e-05,
      "loss": 0.5158,
      "step": 50
    },
    {
      "epoch": 0.029507229271171435,
      "grad_norm": 34.922237396240234,
      "learning_rate": 1.9811153732664505e-05,
      "loss": 0.449,
      "step": 100
    },
    {
      "epoch": 0.04426084390675716,
      "grad_norm": 19.413869857788086,
      "learning_rate": 1.9712796301760598e-05,
      "loss": 0.3807,
      "step": 150
    },
    {
      "epoch": 0.05901445854234287,
      "grad_norm": 15.71127700805664,
      "learning_rate": 1.9614438870856695e-05,
      "loss": 0.3435,
      "step": 200
    },
    {
      "epoch": 0.07376807317792859,
      "grad_norm": 19.735576629638672,
      "learning_rate": 1.951608143995279e-05,
      "loss": 0.3411,
      "step": 250
    },
    {
      "epoch": 0.08852168781351431,
      "grad_norm": 12.763652801513672,
      "learning_rate": 1.9417724009048885e-05,
      "loss": 0.3606,
      "step": 300
    },
    {
      "epoch": 0.10327530244910003,
      "grad_norm": 27.169620513916016,
      "learning_rate": 1.931936657814498e-05,
      "loss": 0.3151,
      "step": 350
    },
    {
      "epoch": 0.11802891708468574,
      "grad_norm": 16.274869918823242,
      "learning_rate": 1.9221009147241078e-05,
      "loss": 0.3388,
      "step": 400
    },
    {
      "epoch": 0.13278253172027146,
      "grad_norm": 17.08152198791504,
      "learning_rate": 1.912265171633717e-05,
      "loss": 0.2806,
      "step": 450
    },
    {
      "epoch": 0.14753614635585718,
      "grad_norm": 15.775869369506836,
      "learning_rate": 1.9024294285433268e-05,
      "loss": 0.2805,
      "step": 500
    },
    {
      "epoch": 0.1622897609914429,
      "grad_norm": 11.408373832702637,
      "learning_rate": 1.892593685452936e-05,
      "loss": 0.2869,
      "step": 550
    },
    {
      "epoch": 0.17704337562702863,
      "grad_norm": 11.551848411560059,
      "learning_rate": 1.8827579423625458e-05,
      "loss": 0.2693,
      "step": 600
    },
    {
      "epoch": 0.19179699026261435,
      "grad_norm": 20.29874610900879,
      "learning_rate": 1.872922199272155e-05,
      "loss": 0.2542,
      "step": 650
    },
    {
      "epoch": 0.20655060489820007,
      "grad_norm": 15.843415260314941,
      "learning_rate": 1.8630864561817648e-05,
      "loss": 0.2888,
      "step": 700
    },
    {
      "epoch": 0.2213042195337858,
      "grad_norm": 15.390551567077637,
      "learning_rate": 1.853250713091374e-05,
      "loss": 0.262,
      "step": 750
    },
    {
      "epoch": 0.23605783416937148,
      "grad_norm": 13.236431121826172,
      "learning_rate": 1.8434149700009838e-05,
      "loss": 0.2814,
      "step": 800
    },
    {
      "epoch": 0.2508114488049572,
      "grad_norm": 10.343287467956543,
      "learning_rate": 1.833579226910593e-05,
      "loss": 0.2735,
      "step": 850
    },
    {
      "epoch": 0.2655650634405429,
      "grad_norm": 11.96174144744873,
      "learning_rate": 1.8237434838202028e-05,
      "loss": 0.2992,
      "step": 900
    },
    {
      "epoch": 0.28031867807612865,
      "grad_norm": 78.47976684570312,
      "learning_rate": 1.8141044555916202e-05,
      "loss": 0.2832,
      "step": 950
    },
    {
      "epoch": 0.29507229271171437,
      "grad_norm": 12.881858825683594,
      "learning_rate": 1.8042687125012295e-05,
      "loss": 0.321,
      "step": 1000
    },
    {
      "epoch": 0.3098259073473001,
      "grad_norm": 25.304340362548828,
      "learning_rate": 1.7944329694108392e-05,
      "loss": 0.2536,
      "step": 1050
    },
    {
      "epoch": 0.3245795219828858,
      "grad_norm": 20.92299461364746,
      "learning_rate": 1.784597226320449e-05,
      "loss": 0.2772,
      "step": 1100
    },
    {
      "epoch": 0.33933313661847153,
      "grad_norm": 10.028778076171875,
      "learning_rate": 1.7747614832300582e-05,
      "loss": 0.2733,
      "step": 1150
    },
    {
      "epoch": 0.35408675125405725,
      "grad_norm": 17.62531852722168,
      "learning_rate": 1.764925740139668e-05,
      "loss": 0.2893,
      "step": 1200
    },
    {
      "epoch": 0.368840365889643,
      "grad_norm": 7.395332336425781,
      "learning_rate": 1.7550899970492772e-05,
      "loss": 0.2928,
      "step": 1250
    },
    {
      "epoch": 0.3835939805252287,
      "grad_norm": 22.992679595947266,
      "learning_rate": 1.745254253958887e-05,
      "loss": 0.2677,
      "step": 1300
    },
    {
      "epoch": 0.3983475951608144,
      "grad_norm": 11.437336921691895,
      "learning_rate": 1.7354185108684962e-05,
      "loss": 0.2623,
      "step": 1350
    },
    {
      "epoch": 0.41310120979640014,
      "grad_norm": 14.038150787353516,
      "learning_rate": 1.725582767778106e-05,
      "loss": 0.2861,
      "step": 1400
    },
    {
      "epoch": 0.42785482443198586,
      "grad_norm": 16.53495216369629,
      "learning_rate": 1.7157470246877152e-05,
      "loss": 0.2675,
      "step": 1450
    },
    {
      "epoch": 0.4426084390675716,
      "grad_norm": 13.95640754699707,
      "learning_rate": 1.705911281597325e-05,
      "loss": 0.2753,
      "step": 1500
    },
    {
      "epoch": 0.4573620537031573,
      "grad_norm": 13.526002883911133,
      "learning_rate": 1.6960755385069342e-05,
      "loss": 0.2553,
      "step": 1550
    },
    {
      "epoch": 0.47211566833874297,
      "grad_norm": 19.354494094848633,
      "learning_rate": 1.686239795416544e-05,
      "loss": 0.2763,
      "step": 1600
    },
    {
      "epoch": 0.4868692829743287,
      "grad_norm": 13.959134101867676,
      "learning_rate": 1.6764040523261532e-05,
      "loss": 0.2574,
      "step": 1650
    },
    {
      "epoch": 0.5016228976099144,
      "grad_norm": 18.805553436279297,
      "learning_rate": 1.666568309235763e-05,
      "loss": 0.258,
      "step": 1700
    },
    {
      "epoch": 0.5163765122455002,
      "grad_norm": 23.37970542907715,
      "learning_rate": 1.6567325661453722e-05,
      "loss": 0.2518,
      "step": 1750
    },
    {
      "epoch": 0.5311301268810859,
      "grad_norm": 10.360712051391602,
      "learning_rate": 1.646896823054982e-05,
      "loss": 0.2462,
      "step": 1800
    },
    {
      "epoch": 0.5458837415166716,
      "grad_norm": 11.004132270812988,
      "learning_rate": 1.6370610799645915e-05,
      "loss": 0.2461,
      "step": 1850
    },
    {
      "epoch": 0.5606373561522573,
      "grad_norm": 11.803858757019043,
      "learning_rate": 1.627225336874201e-05,
      "loss": 0.2664,
      "step": 1900
    },
    {
      "epoch": 0.5753909707878431,
      "grad_norm": 22.3846378326416,
      "learning_rate": 1.6173895937838105e-05,
      "loss": 0.2566,
      "step": 1950
    },
    {
      "epoch": 0.5901445854234287,
      "grad_norm": 15.644088745117188,
      "learning_rate": 1.6075538506934202e-05,
      "loss": 0.2479,
      "step": 2000
    },
    {
      "epoch": 0.6048982000590145,
      "grad_norm": 8.440351486206055,
      "learning_rate": 1.5977181076030295e-05,
      "loss": 0.2306,
      "step": 2050
    },
    {
      "epoch": 0.6196518146946002,
      "grad_norm": 21.115657806396484,
      "learning_rate": 1.5878823645126392e-05,
      "loss": 0.2331,
      "step": 2100
    },
    {
      "epoch": 0.6344054293301858,
      "grad_norm": 17.30714225769043,
      "learning_rate": 1.5780466214222485e-05,
      "loss": 0.2645,
      "step": 2150
    },
    {
      "epoch": 0.6491590439657716,
      "grad_norm": 9.28238296508789,
      "learning_rate": 1.5682108783318582e-05,
      "loss": 0.2353,
      "step": 2200
    },
    {
      "epoch": 0.6639126586013573,
      "grad_norm": 10.211525917053223,
      "learning_rate": 1.5583751352414675e-05,
      "loss": 0.2395,
      "step": 2250
    },
    {
      "epoch": 0.6786662732369431,
      "grad_norm": 13.77010726928711,
      "learning_rate": 1.5485393921510772e-05,
      "loss": 0.2269,
      "step": 2300
    },
    {
      "epoch": 0.6934198878725287,
      "grad_norm": 11.942183494567871,
      "learning_rate": 1.5387036490606865e-05,
      "loss": 0.2033,
      "step": 2350
    },
    {
      "epoch": 0.7081735025081145,
      "grad_norm": 11.766552925109863,
      "learning_rate": 1.5288679059702962e-05,
      "loss": 0.2343,
      "step": 2400
    },
    {
      "epoch": 0.7229271171437002,
      "grad_norm": 11.78639030456543,
      "learning_rate": 1.5190321628799055e-05,
      "loss": 0.2314,
      "step": 2450
    },
    {
      "epoch": 0.737680731779286,
      "grad_norm": 12.706385612487793,
      "learning_rate": 1.5091964197895152e-05,
      "loss": 0.2146,
      "step": 2500
    },
    {
      "epoch": 0.7524343464148716,
      "grad_norm": 21.73746681213379,
      "learning_rate": 1.4993606766991247e-05,
      "loss": 0.2162,
      "step": 2550
    },
    {
      "epoch": 0.7671879610504574,
      "grad_norm": 7.380034446716309,
      "learning_rate": 1.4895249336087342e-05,
      "loss": 0.2236,
      "step": 2600
    },
    {
      "epoch": 0.7819415756860431,
      "grad_norm": 25.833574295043945,
      "learning_rate": 1.4796891905183437e-05,
      "loss": 0.2319,
      "step": 2650
    },
    {
      "epoch": 0.7966951903216288,
      "grad_norm": 26.331480026245117,
      "learning_rate": 1.4698534474279534e-05,
      "loss": 0.2625,
      "step": 2700
    },
    {
      "epoch": 0.8114488049572145,
      "grad_norm": 8.645496368408203,
      "learning_rate": 1.4600177043375627e-05,
      "loss": 0.208,
      "step": 2750
    },
    {
      "epoch": 0.8262024195928003,
      "grad_norm": 12.316970825195312,
      "learning_rate": 1.4501819612471724e-05,
      "loss": 0.2237,
      "step": 2800
    },
    {
      "epoch": 0.8409560342283859,
      "grad_norm": 13.595348358154297,
      "learning_rate": 1.4403462181567817e-05,
      "loss": 0.2209,
      "step": 2850
    },
    {
      "epoch": 0.8557096488639717,
      "grad_norm": 10.125897407531738,
      "learning_rate": 1.4305104750663914e-05,
      "loss": 0.219,
      "step": 2900
    },
    {
      "epoch": 0.8704632634995574,
      "grad_norm": 13.125916481018066,
      "learning_rate": 1.4206747319760009e-05,
      "loss": 0.2398,
      "step": 2950
    },
    {
      "epoch": 0.8852168781351432,
      "grad_norm": 10.629365921020508,
      "learning_rate": 1.4110357037474183e-05,
      "loss": 0.2158,
      "step": 3000
    },
    {
      "epoch": 0.8999704927707288,
      "grad_norm": 10.521378517150879,
      "learning_rate": 1.4011999606570276e-05,
      "loss": 0.2171,
      "step": 3050
    },
    {
      "epoch": 0.9147241074063146,
      "grad_norm": 15.863011360168457,
      "learning_rate": 1.3913642175666373e-05,
      "loss": 0.2199,
      "step": 3100
    },
    {
      "epoch": 0.9294777220419003,
      "grad_norm": 27.00796127319336,
      "learning_rate": 1.3815284744762468e-05,
      "loss": 0.2116,
      "step": 3150
    },
    {
      "epoch": 0.9442313366774859,
      "grad_norm": 11.89793872833252,
      "learning_rate": 1.3716927313858563e-05,
      "loss": 0.21,
      "step": 3200
    },
    {
      "epoch": 0.9589849513130717,
      "grad_norm": 12.05441951751709,
      "learning_rate": 1.3618569882954658e-05,
      "loss": 0.2004,
      "step": 3250
    },
    {
      "epoch": 0.9737385659486574,
      "grad_norm": 11.84296703338623,
      "learning_rate": 1.3520212452050755e-05,
      "loss": 0.2172,
      "step": 3300
    },
    {
      "epoch": 0.9884921805842432,
      "grad_norm": 13.233684539794922,
      "learning_rate": 1.3421855021146848e-05,
      "loss": 0.2067,
      "step": 3350
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8841841251106521,
      "eval_f1": 0.8175903334495178,
      "eval_loss": 0.2720773220062256,
      "eval_precision": 0.9670193761165315,
      "eval_recall": 0.7081614169266378,
      "eval_runtime": 66.4713,
      "eval_samples_per_second": 407.875,
      "eval_steps_per_second": 12.757,
      "step": 3389
    }
  ],
  "logging_steps": 50,
  "max_steps": 10167,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 647901436815360.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}

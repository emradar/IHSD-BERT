{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 6778,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014753614635585718,
      "grad_norm": 35.95878601074219,
      "learning_rate": 1.990754401495033e-05,
      "loss": 0.5158,
      "step": 50
    },
    {
      "epoch": 0.029507229271171435,
      "grad_norm": 34.922237396240234,
      "learning_rate": 1.9811153732664505e-05,
      "loss": 0.449,
      "step": 100
    },
    {
      "epoch": 0.04426084390675716,
      "grad_norm": 19.413869857788086,
      "learning_rate": 1.9712796301760598e-05,
      "loss": 0.3807,
      "step": 150
    },
    {
      "epoch": 0.05901445854234287,
      "grad_norm": 15.71127700805664,
      "learning_rate": 1.9614438870856695e-05,
      "loss": 0.3435,
      "step": 200
    },
    {
      "epoch": 0.07376807317792859,
      "grad_norm": 19.735576629638672,
      "learning_rate": 1.951608143995279e-05,
      "loss": 0.3411,
      "step": 250
    },
    {
      "epoch": 0.08852168781351431,
      "grad_norm": 12.763652801513672,
      "learning_rate": 1.9417724009048885e-05,
      "loss": 0.3606,
      "step": 300
    },
    {
      "epoch": 0.10327530244910003,
      "grad_norm": 27.169620513916016,
      "learning_rate": 1.931936657814498e-05,
      "loss": 0.3151,
      "step": 350
    },
    {
      "epoch": 0.11802891708468574,
      "grad_norm": 16.274869918823242,
      "learning_rate": 1.9221009147241078e-05,
      "loss": 0.3388,
      "step": 400
    },
    {
      "epoch": 0.13278253172027146,
      "grad_norm": 17.08152198791504,
      "learning_rate": 1.912265171633717e-05,
      "loss": 0.2806,
      "step": 450
    },
    {
      "epoch": 0.14753614635585718,
      "grad_norm": 15.775869369506836,
      "learning_rate": 1.9024294285433268e-05,
      "loss": 0.2805,
      "step": 500
    },
    {
      "epoch": 0.1622897609914429,
      "grad_norm": 11.408373832702637,
      "learning_rate": 1.892593685452936e-05,
      "loss": 0.2869,
      "step": 550
    },
    {
      "epoch": 0.17704337562702863,
      "grad_norm": 11.551848411560059,
      "learning_rate": 1.8827579423625458e-05,
      "loss": 0.2693,
      "step": 600
    },
    {
      "epoch": 0.19179699026261435,
      "grad_norm": 20.29874610900879,
      "learning_rate": 1.872922199272155e-05,
      "loss": 0.2542,
      "step": 650
    },
    {
      "epoch": 0.20655060489820007,
      "grad_norm": 15.843415260314941,
      "learning_rate": 1.8630864561817648e-05,
      "loss": 0.2888,
      "step": 700
    },
    {
      "epoch": 0.2213042195337858,
      "grad_norm": 15.390551567077637,
      "learning_rate": 1.853250713091374e-05,
      "loss": 0.262,
      "step": 750
    },
    {
      "epoch": 0.23605783416937148,
      "grad_norm": 13.236431121826172,
      "learning_rate": 1.8434149700009838e-05,
      "loss": 0.2814,
      "step": 800
    },
    {
      "epoch": 0.2508114488049572,
      "grad_norm": 10.343287467956543,
      "learning_rate": 1.833579226910593e-05,
      "loss": 0.2735,
      "step": 850
    },
    {
      "epoch": 0.2655650634405429,
      "grad_norm": 11.96174144744873,
      "learning_rate": 1.8237434838202028e-05,
      "loss": 0.2992,
      "step": 900
    },
    {
      "epoch": 0.28031867807612865,
      "grad_norm": 78.47976684570312,
      "learning_rate": 1.8141044555916202e-05,
      "loss": 0.2832,
      "step": 950
    },
    {
      "epoch": 0.29507229271171437,
      "grad_norm": 12.881858825683594,
      "learning_rate": 1.8042687125012295e-05,
      "loss": 0.321,
      "step": 1000
    },
    {
      "epoch": 0.3098259073473001,
      "grad_norm": 25.304340362548828,
      "learning_rate": 1.7944329694108392e-05,
      "loss": 0.2536,
      "step": 1050
    },
    {
      "epoch": 0.3245795219828858,
      "grad_norm": 20.92299461364746,
      "learning_rate": 1.784597226320449e-05,
      "loss": 0.2772,
      "step": 1100
    },
    {
      "epoch": 0.33933313661847153,
      "grad_norm": 10.028778076171875,
      "learning_rate": 1.7747614832300582e-05,
      "loss": 0.2733,
      "step": 1150
    },
    {
      "epoch": 0.35408675125405725,
      "grad_norm": 17.62531852722168,
      "learning_rate": 1.764925740139668e-05,
      "loss": 0.2893,
      "step": 1200
    },
    {
      "epoch": 0.368840365889643,
      "grad_norm": 7.395332336425781,
      "learning_rate": 1.7550899970492772e-05,
      "loss": 0.2928,
      "step": 1250
    },
    {
      "epoch": 0.3835939805252287,
      "grad_norm": 22.992679595947266,
      "learning_rate": 1.745254253958887e-05,
      "loss": 0.2677,
      "step": 1300
    },
    {
      "epoch": 0.3983475951608144,
      "grad_norm": 11.437336921691895,
      "learning_rate": 1.7354185108684962e-05,
      "loss": 0.2623,
      "step": 1350
    },
    {
      "epoch": 0.41310120979640014,
      "grad_norm": 14.038150787353516,
      "learning_rate": 1.725582767778106e-05,
      "loss": 0.2861,
      "step": 1400
    },
    {
      "epoch": 0.42785482443198586,
      "grad_norm": 16.53495216369629,
      "learning_rate": 1.7157470246877152e-05,
      "loss": 0.2675,
      "step": 1450
    },
    {
      "epoch": 0.4426084390675716,
      "grad_norm": 13.95640754699707,
      "learning_rate": 1.705911281597325e-05,
      "loss": 0.2753,
      "step": 1500
    },
    {
      "epoch": 0.4573620537031573,
      "grad_norm": 13.526002883911133,
      "learning_rate": 1.6960755385069342e-05,
      "loss": 0.2553,
      "step": 1550
    },
    {
      "epoch": 0.47211566833874297,
      "grad_norm": 19.354494094848633,
      "learning_rate": 1.686239795416544e-05,
      "loss": 0.2763,
      "step": 1600
    },
    {
      "epoch": 0.4868692829743287,
      "grad_norm": 13.959134101867676,
      "learning_rate": 1.6764040523261532e-05,
      "loss": 0.2574,
      "step": 1650
    },
    {
      "epoch": 0.5016228976099144,
      "grad_norm": 18.805553436279297,
      "learning_rate": 1.666568309235763e-05,
      "loss": 0.258,
      "step": 1700
    },
    {
      "epoch": 0.5163765122455002,
      "grad_norm": 23.37970542907715,
      "learning_rate": 1.6567325661453722e-05,
      "loss": 0.2518,
      "step": 1750
    },
    {
      "epoch": 0.5311301268810859,
      "grad_norm": 10.360712051391602,
      "learning_rate": 1.646896823054982e-05,
      "loss": 0.2462,
      "step": 1800
    },
    {
      "epoch": 0.5458837415166716,
      "grad_norm": 11.004132270812988,
      "learning_rate": 1.6370610799645915e-05,
      "loss": 0.2461,
      "step": 1850
    },
    {
      "epoch": 0.5606373561522573,
      "grad_norm": 11.803858757019043,
      "learning_rate": 1.627225336874201e-05,
      "loss": 0.2664,
      "step": 1900
    },
    {
      "epoch": 0.5753909707878431,
      "grad_norm": 22.3846378326416,
      "learning_rate": 1.6173895937838105e-05,
      "loss": 0.2566,
      "step": 1950
    },
    {
      "epoch": 0.5901445854234287,
      "grad_norm": 15.644088745117188,
      "learning_rate": 1.6075538506934202e-05,
      "loss": 0.2479,
      "step": 2000
    },
    {
      "epoch": 0.6048982000590145,
      "grad_norm": 8.440351486206055,
      "learning_rate": 1.5977181076030295e-05,
      "loss": 0.2306,
      "step": 2050
    },
    {
      "epoch": 0.6196518146946002,
      "grad_norm": 21.115657806396484,
      "learning_rate": 1.5878823645126392e-05,
      "loss": 0.2331,
      "step": 2100
    },
    {
      "epoch": 0.6344054293301858,
      "grad_norm": 17.30714225769043,
      "learning_rate": 1.5780466214222485e-05,
      "loss": 0.2645,
      "step": 2150
    },
    {
      "epoch": 0.6491590439657716,
      "grad_norm": 9.28238296508789,
      "learning_rate": 1.5682108783318582e-05,
      "loss": 0.2353,
      "step": 2200
    },
    {
      "epoch": 0.6639126586013573,
      "grad_norm": 10.211525917053223,
      "learning_rate": 1.5583751352414675e-05,
      "loss": 0.2395,
      "step": 2250
    },
    {
      "epoch": 0.6786662732369431,
      "grad_norm": 13.77010726928711,
      "learning_rate": 1.5485393921510772e-05,
      "loss": 0.2269,
      "step": 2300
    },
    {
      "epoch": 0.6934198878725287,
      "grad_norm": 11.942183494567871,
      "learning_rate": 1.5387036490606865e-05,
      "loss": 0.2033,
      "step": 2350
    },
    {
      "epoch": 0.7081735025081145,
      "grad_norm": 11.766552925109863,
      "learning_rate": 1.5288679059702962e-05,
      "loss": 0.2343,
      "step": 2400
    },
    {
      "epoch": 0.7229271171437002,
      "grad_norm": 11.78639030456543,
      "learning_rate": 1.5190321628799055e-05,
      "loss": 0.2314,
      "step": 2450
    },
    {
      "epoch": 0.737680731779286,
      "grad_norm": 12.706385612487793,
      "learning_rate": 1.5091964197895152e-05,
      "loss": 0.2146,
      "step": 2500
    },
    {
      "epoch": 0.7524343464148716,
      "grad_norm": 21.73746681213379,
      "learning_rate": 1.4993606766991247e-05,
      "loss": 0.2162,
      "step": 2550
    },
    {
      "epoch": 0.7671879610504574,
      "grad_norm": 7.380034446716309,
      "learning_rate": 1.4895249336087342e-05,
      "loss": 0.2236,
      "step": 2600
    },
    {
      "epoch": 0.7819415756860431,
      "grad_norm": 25.833574295043945,
      "learning_rate": 1.4796891905183437e-05,
      "loss": 0.2319,
      "step": 2650
    },
    {
      "epoch": 0.7966951903216288,
      "grad_norm": 26.331480026245117,
      "learning_rate": 1.4698534474279534e-05,
      "loss": 0.2625,
      "step": 2700
    },
    {
      "epoch": 0.8114488049572145,
      "grad_norm": 8.645496368408203,
      "learning_rate": 1.4600177043375627e-05,
      "loss": 0.208,
      "step": 2750
    },
    {
      "epoch": 0.8262024195928003,
      "grad_norm": 12.316970825195312,
      "learning_rate": 1.4501819612471724e-05,
      "loss": 0.2237,
      "step": 2800
    },
    {
      "epoch": 0.8409560342283859,
      "grad_norm": 13.595348358154297,
      "learning_rate": 1.4403462181567817e-05,
      "loss": 0.2209,
      "step": 2850
    },
    {
      "epoch": 0.8557096488639717,
      "grad_norm": 10.125897407531738,
      "learning_rate": 1.4305104750663914e-05,
      "loss": 0.219,
      "step": 2900
    },
    {
      "epoch": 0.8704632634995574,
      "grad_norm": 13.125916481018066,
      "learning_rate": 1.4206747319760009e-05,
      "loss": 0.2398,
      "step": 2950
    },
    {
      "epoch": 0.8852168781351432,
      "grad_norm": 10.629365921020508,
      "learning_rate": 1.4110357037474183e-05,
      "loss": 0.2158,
      "step": 3000
    },
    {
      "epoch": 0.8999704927707288,
      "grad_norm": 10.521378517150879,
      "learning_rate": 1.4011999606570276e-05,
      "loss": 0.2171,
      "step": 3050
    },
    {
      "epoch": 0.9147241074063146,
      "grad_norm": 15.863011360168457,
      "learning_rate": 1.3913642175666373e-05,
      "loss": 0.2199,
      "step": 3100
    },
    {
      "epoch": 0.9294777220419003,
      "grad_norm": 27.00796127319336,
      "learning_rate": 1.3815284744762468e-05,
      "loss": 0.2116,
      "step": 3150
    },
    {
      "epoch": 0.9442313366774859,
      "grad_norm": 11.89793872833252,
      "learning_rate": 1.3716927313858563e-05,
      "loss": 0.21,
      "step": 3200
    },
    {
      "epoch": 0.9589849513130717,
      "grad_norm": 12.05441951751709,
      "learning_rate": 1.3618569882954658e-05,
      "loss": 0.2004,
      "step": 3250
    },
    {
      "epoch": 0.9737385659486574,
      "grad_norm": 11.84296703338623,
      "learning_rate": 1.3520212452050755e-05,
      "loss": 0.2172,
      "step": 3300
    },
    {
      "epoch": 0.9884921805842432,
      "grad_norm": 13.233684539794922,
      "learning_rate": 1.3421855021146848e-05,
      "loss": 0.2067,
      "step": 3350
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8841841251106521,
      "eval_f1": 0.8175903334495178,
      "eval_loss": 0.2720773220062256,
      "eval_precision": 0.9670193761165315,
      "eval_recall": 0.7081614169266378,
      "eval_runtime": 66.4713,
      "eval_samples_per_second": 407.875,
      "eval_steps_per_second": 12.757,
      "step": 3389
    },
    {
      "epoch": 1.0032457952198288,
      "grad_norm": 15.707645416259766,
      "learning_rate": 1.3323497590242945e-05,
      "loss": 0.2302,
      "step": 3400
    },
    {
      "epoch": 1.0179994098554146,
      "grad_norm": 10.234325408935547,
      "learning_rate": 1.3225140159339038e-05,
      "loss": 0.1962,
      "step": 3450
    },
    {
      "epoch": 1.0327530244910004,
      "grad_norm": 31.756710052490234,
      "learning_rate": 1.3126782728435135e-05,
      "loss": 0.18,
      "step": 3500
    },
    {
      "epoch": 1.047506639126586,
      "grad_norm": 17.46403694152832,
      "learning_rate": 1.3028425297531228e-05,
      "loss": 0.1766,
      "step": 3550
    },
    {
      "epoch": 1.0622602537621717,
      "grad_norm": 19.273591995239258,
      "learning_rate": 1.2930067866627325e-05,
      "loss": 0.1683,
      "step": 3600
    },
    {
      "epoch": 1.0770138683977575,
      "grad_norm": 15.205924034118652,
      "learning_rate": 1.2833677584341497e-05,
      "loss": 0.1664,
      "step": 3650
    },
    {
      "epoch": 1.0917674830333433,
      "grad_norm": 11.952672004699707,
      "learning_rate": 1.2735320153437594e-05,
      "loss": 0.1809,
      "step": 3700
    },
    {
      "epoch": 1.1065210976689288,
      "grad_norm": 19.423105239868164,
      "learning_rate": 1.2636962722533687e-05,
      "loss": 0.1644,
      "step": 3750
    },
    {
      "epoch": 1.1212747123045146,
      "grad_norm": 22.89329719543457,
      "learning_rate": 1.2538605291629784e-05,
      "loss": 0.1705,
      "step": 3800
    },
    {
      "epoch": 1.1360283269401004,
      "grad_norm": 27.065406799316406,
      "learning_rate": 1.2440247860725879e-05,
      "loss": 0.1691,
      "step": 3850
    },
    {
      "epoch": 1.1507819415756861,
      "grad_norm": 29.051725387573242,
      "learning_rate": 1.2341890429821974e-05,
      "loss": 0.1856,
      "step": 3900
    },
    {
      "epoch": 1.1655355562112717,
      "grad_norm": 16.410865783691406,
      "learning_rate": 1.2243532998918069e-05,
      "loss": 0.1572,
      "step": 3950
    },
    {
      "epoch": 1.1802891708468575,
      "grad_norm": 8.293124198913574,
      "learning_rate": 1.2145175568014165e-05,
      "loss": 0.1893,
      "step": 4000
    },
    {
      "epoch": 1.1950427854824432,
      "grad_norm": 23.799272537231445,
      "learning_rate": 1.2046818137110259e-05,
      "loss": 0.1773,
      "step": 4050
    },
    {
      "epoch": 1.2097964001180288,
      "grad_norm": 19.954345703125,
      "learning_rate": 1.1948460706206355e-05,
      "loss": 0.1708,
      "step": 4100
    },
    {
      "epoch": 1.2245500147536146,
      "grad_norm": 24.018823623657227,
      "learning_rate": 1.1850103275302449e-05,
      "loss": 0.1643,
      "step": 4150
    },
    {
      "epoch": 1.2393036293892004,
      "grad_norm": 18.422529220581055,
      "learning_rate": 1.1751745844398545e-05,
      "loss": 0.1628,
      "step": 4200
    },
    {
      "epoch": 1.2540572440247861,
      "grad_norm": 15.990468978881836,
      "learning_rate": 1.1653388413494639e-05,
      "loss": 0.1624,
      "step": 4250
    },
    {
      "epoch": 1.268810858660372,
      "grad_norm": 19.795337677001953,
      "learning_rate": 1.1555030982590735e-05,
      "loss": 0.1421,
      "step": 4300
    },
    {
      "epoch": 1.2835644732959575,
      "grad_norm": 15.001262664794922,
      "learning_rate": 1.145667355168683e-05,
      "loss": 0.1667,
      "step": 4350
    },
    {
      "epoch": 1.2983180879315432,
      "grad_norm": 33.471092224121094,
      "learning_rate": 1.1358316120782925e-05,
      "loss": 0.1553,
      "step": 4400
    },
    {
      "epoch": 1.313071702567129,
      "grad_norm": 15.872535705566406,
      "learning_rate": 1.125995868987902e-05,
      "loss": 0.1782,
      "step": 4450
    },
    {
      "epoch": 1.3278253172027146,
      "grad_norm": 29.962173461914062,
      "learning_rate": 1.1161601258975117e-05,
      "loss": 0.1476,
      "step": 4500
    },
    {
      "epoch": 1.3425789318383003,
      "grad_norm": 21.260520935058594,
      "learning_rate": 1.106324382807121e-05,
      "loss": 0.1602,
      "step": 4550
    },
    {
      "epoch": 1.3573325464738861,
      "grad_norm": 6.530350685119629,
      "learning_rate": 1.0964886397167307e-05,
      "loss": 0.1529,
      "step": 4600
    },
    {
      "epoch": 1.372086161109472,
      "grad_norm": 19.687225341796875,
      "learning_rate": 1.08665289662634e-05,
      "loss": 0.1459,
      "step": 4650
    },
    {
      "epoch": 1.3868397757450575,
      "grad_norm": 19.513004302978516,
      "learning_rate": 1.0768171535359497e-05,
      "loss": 0.1294,
      "step": 4700
    },
    {
      "epoch": 1.4015933903806432,
      "grad_norm": 21.108739852905273,
      "learning_rate": 1.0669814104455592e-05,
      "loss": 0.1547,
      "step": 4750
    },
    {
      "epoch": 1.416347005016229,
      "grad_norm": 6.001799583435059,
      "learning_rate": 1.0571456673551687e-05,
      "loss": 0.1629,
      "step": 4800
    },
    {
      "epoch": 1.4311006196518146,
      "grad_norm": 16.36135482788086,
      "learning_rate": 1.0473099242647784e-05,
      "loss": 0.1732,
      "step": 4850
    },
    {
      "epoch": 1.4458542342874003,
      "grad_norm": 5.757575988769531,
      "learning_rate": 1.0374741811743879e-05,
      "loss": 0.1445,
      "step": 4900
    },
    {
      "epoch": 1.4606078489229861,
      "grad_norm": 13.592549324035645,
      "learning_rate": 1.0276384380839974e-05,
      "loss": 0.1355,
      "step": 4950
    },
    {
      "epoch": 1.475361463558572,
      "grad_norm": 27.45326042175293,
      "learning_rate": 1.0178026949936069e-05,
      "loss": 0.1633,
      "step": 5000
    },
    {
      "epoch": 1.4901150781941577,
      "grad_norm": 32.57160568237305,
      "learning_rate": 1.0079669519032166e-05,
      "loss": 0.1537,
      "step": 5050
    },
    {
      "epoch": 1.5048686928297434,
      "grad_norm": 4.378874778747559,
      "learning_rate": 9.981312088128259e-06,
      "loss": 0.1377,
      "step": 5100
    },
    {
      "epoch": 1.519622307465329,
      "grad_norm": 26.49330711364746,
      "learning_rate": 9.882954657224354e-06,
      "loss": 0.1394,
      "step": 5150
    },
    {
      "epoch": 1.5343759221009146,
      "grad_norm": 12.735922813415527,
      "learning_rate": 9.784597226320449e-06,
      "loss": 0.156,
      "step": 5200
    },
    {
      "epoch": 1.5491295367365003,
      "grad_norm": 26.95480728149414,
      "learning_rate": 9.686239795416544e-06,
      "loss": 0.1346,
      "step": 5250
    },
    {
      "epoch": 1.5638831513720861,
      "grad_norm": 19.728858947753906,
      "learning_rate": 9.587882364512639e-06,
      "loss": 0.1219,
      "step": 5300
    },
    {
      "epoch": 1.5786367660076719,
      "grad_norm": 35.20137023925781,
      "learning_rate": 9.489524933608736e-06,
      "loss": 0.162,
      "step": 5350
    },
    {
      "epoch": 1.5933903806432577,
      "grad_norm": 15.345378875732422,
      "learning_rate": 9.39116750270483e-06,
      "loss": 0.155,
      "step": 5400
    },
    {
      "epoch": 1.6081439952788434,
      "grad_norm": 8.416797637939453,
      "learning_rate": 9.292810071800926e-06,
      "loss": 0.1305,
      "step": 5450
    },
    {
      "epoch": 1.622897609914429,
      "grad_norm": 13.005109786987305,
      "learning_rate": 9.19445264089702e-06,
      "loss": 0.1579,
      "step": 5500
    },
    {
      "epoch": 1.6376512245500148,
      "grad_norm": 57.68626403808594,
      "learning_rate": 9.096095209993116e-06,
      "loss": 0.164,
      "step": 5550
    },
    {
      "epoch": 1.6524048391856003,
      "grad_norm": 12.094625473022461,
      "learning_rate": 8.99773777908921e-06,
      "loss": 0.1188,
      "step": 5600
    },
    {
      "epoch": 1.667158453821186,
      "grad_norm": 7.771948337554932,
      "learning_rate": 8.899380348185306e-06,
      "loss": 0.1263,
      "step": 5650
    },
    {
      "epoch": 1.6819120684567719,
      "grad_norm": 17.861209869384766,
      "learning_rate": 8.8010229172814e-06,
      "loss": 0.1537,
      "step": 5700
    },
    {
      "epoch": 1.6966656830923577,
      "grad_norm": 40.418949127197266,
      "learning_rate": 8.702665486377496e-06,
      "loss": 0.1379,
      "step": 5750
    },
    {
      "epoch": 1.7114192977279434,
      "grad_norm": 13.165390014648438,
      "learning_rate": 8.60430805547359e-06,
      "loss": 0.1381,
      "step": 5800
    },
    {
      "epoch": 1.7261729123635292,
      "grad_norm": 16.610275268554688,
      "learning_rate": 8.505950624569687e-06,
      "loss": 0.131,
      "step": 5850
    },
    {
      "epoch": 1.7409265269991148,
      "grad_norm": 30.30884552001953,
      "learning_rate": 8.407593193665782e-06,
      "loss": 0.134,
      "step": 5900
    },
    {
      "epoch": 1.7556801416347005,
      "grad_norm": 18.740467071533203,
      "learning_rate": 8.309235762761877e-06,
      "loss": 0.1103,
      "step": 5950
    },
    {
      "epoch": 1.770433756270286,
      "grad_norm": 49.667110443115234,
      "learning_rate": 8.210878331857972e-06,
      "loss": 0.1395,
      "step": 6000
    },
    {
      "epoch": 1.7851873709058719,
      "grad_norm": 8.648212432861328,
      "learning_rate": 8.112520900954067e-06,
      "loss": 0.1335,
      "step": 6050
    },
    {
      "epoch": 1.7999409855414576,
      "grad_norm": 16.187158584594727,
      "learning_rate": 8.014163470050162e-06,
      "loss": 0.1315,
      "step": 6100
    },
    {
      "epoch": 1.8146946001770434,
      "grad_norm": 27.08147430419922,
      "learning_rate": 7.915806039146257e-06,
      "loss": 0.1198,
      "step": 6150
    },
    {
      "epoch": 1.8294482148126292,
      "grad_norm": 38.756370544433594,
      "learning_rate": 7.817448608242352e-06,
      "loss": 0.1332,
      "step": 6200
    },
    {
      "epoch": 1.8442018294482148,
      "grad_norm": 21.92097282409668,
      "learning_rate": 7.719091177338447e-06,
      "loss": 0.1251,
      "step": 6250
    },
    {
      "epoch": 1.8589554440838005,
      "grad_norm": 12.851887702941895,
      "learning_rate": 7.620733746434543e-06,
      "loss": 0.1249,
      "step": 6300
    },
    {
      "epoch": 1.873709058719386,
      "grad_norm": 20.829137802124023,
      "learning_rate": 7.522376315530638e-06,
      "loss": 0.1274,
      "step": 6350
    },
    {
      "epoch": 1.8884626733549719,
      "grad_norm": 17.043262481689453,
      "learning_rate": 7.424018884626735e-06,
      "loss": 0.1255,
      "step": 6400
    },
    {
      "epoch": 1.9032162879905576,
      "grad_norm": 10.574225425720215,
      "learning_rate": 7.32566145372283e-06,
      "loss": 0.1172,
      "step": 6450
    },
    {
      "epoch": 1.9179699026261434,
      "grad_norm": 21.384891510009766,
      "learning_rate": 7.227304022818925e-06,
      "loss": 0.1152,
      "step": 6500
    },
    {
      "epoch": 1.9327235172617292,
      "grad_norm": 32.91265869140625,
      "learning_rate": 7.128946591915021e-06,
      "loss": 0.1241,
      "step": 6550
    },
    {
      "epoch": 1.947477131897315,
      "grad_norm": 21.573078155517578,
      "learning_rate": 7.030589161011116e-06,
      "loss": 0.1198,
      "step": 6600
    },
    {
      "epoch": 1.9622307465329005,
      "grad_norm": 29.963581085205078,
      "learning_rate": 6.932231730107211e-06,
      "loss": 0.1216,
      "step": 6650
    },
    {
      "epoch": 1.9769843611684863,
      "grad_norm": 27.767959594726562,
      "learning_rate": 6.833874299203306e-06,
      "loss": 0.1131,
      "step": 6700
    },
    {
      "epoch": 1.9917379758040719,
      "grad_norm": 34.19611358642578,
      "learning_rate": 6.735516868299401e-06,
      "loss": 0.1123,
      "step": 6750
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9503172027146651,
      "eval_f1": 0.9305132834665979,
      "eval_loss": 0.1475989669561386,
      "eval_precision": 0.9545935647756139,
      "eval_recall": 0.9076179933581564,
      "eval_runtime": 67.899,
      "eval_samples_per_second": 399.299,
      "eval_steps_per_second": 12.489,
      "step": 6778
    }
  ],
  "logging_steps": 50,
  "max_steps": 10167,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1295802873630720.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}

{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0,
  "eval_steps": 500,
  "global_step": 6778,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.014753614635585718,
      "grad_norm": 45.20372772216797,
      "learning_rate": 1.9930658011212748e-05,
      "loss": 0.5115,
      "step": 50
    },
    {
      "epoch": 0.029507229271171435,
      "grad_norm": 31.487777709960938,
      "learning_rate": 1.985836529949838e-05,
      "loss": 0.4032,
      "step": 100
    },
    {
      "epoch": 0.04426084390675716,
      "grad_norm": 13.647480010986328,
      "learning_rate": 1.978459722632045e-05,
      "loss": 0.3669,
      "step": 150
    },
    {
      "epoch": 0.05901445854234287,
      "grad_norm": 18.305370330810547,
      "learning_rate": 1.971082915314252e-05,
      "loss": 0.3361,
      "step": 200
    },
    {
      "epoch": 0.07376807317792859,
      "grad_norm": 8.100105285644531,
      "learning_rate": 1.963706107996459e-05,
      "loss": 0.3242,
      "step": 250
    },
    {
      "epoch": 0.08852168781351431,
      "grad_norm": 23.703289031982422,
      "learning_rate": 1.9563293006786662e-05,
      "loss": 0.3331,
      "step": 300
    },
    {
      "epoch": 0.10327530244910003,
      "grad_norm": 16.786855697631836,
      "learning_rate": 1.9489524933608736e-05,
      "loss": 0.3054,
      "step": 350
    },
    {
      "epoch": 0.11802891708468574,
      "grad_norm": 29.297719955444336,
      "learning_rate": 1.9415756860430807e-05,
      "loss": 0.3032,
      "step": 400
    },
    {
      "epoch": 0.13278253172027146,
      "grad_norm": 7.438453197479248,
      "learning_rate": 1.9341988787252878e-05,
      "loss": 0.2781,
      "step": 450
    },
    {
      "epoch": 0.14753614635585718,
      "grad_norm": 20.17772102355957,
      "learning_rate": 1.926822071407495e-05,
      "loss": 0.2886,
      "step": 500
    },
    {
      "epoch": 0.1622897609914429,
      "grad_norm": 28.54817771911621,
      "learning_rate": 1.919445264089702e-05,
      "loss": 0.297,
      "step": 550
    },
    {
      "epoch": 0.17704337562702863,
      "grad_norm": 18.9554443359375,
      "learning_rate": 1.9120684567719094e-05,
      "loss": 0.2682,
      "step": 600
    },
    {
      "epoch": 0.19179699026261435,
      "grad_norm": 23.837806701660156,
      "learning_rate": 1.9046916494541165e-05,
      "loss": 0.2616,
      "step": 650
    },
    {
      "epoch": 0.20655060489820007,
      "grad_norm": 14.594475746154785,
      "learning_rate": 1.8973148421363236e-05,
      "loss": 0.2973,
      "step": 700
    },
    {
      "epoch": 0.2213042195337858,
      "grad_norm": 10.83931827545166,
      "learning_rate": 1.8899380348185306e-05,
      "loss": 0.2688,
      "step": 750
    },
    {
      "epoch": 0.23605783416937148,
      "grad_norm": 20.457040786743164,
      "learning_rate": 1.8825612275007377e-05,
      "loss": 0.2784,
      "step": 800
    },
    {
      "epoch": 0.2508114488049572,
      "grad_norm": 21.230304718017578,
      "learning_rate": 1.875184420182945e-05,
      "loss": 0.2657,
      "step": 850
    },
    {
      "epoch": 0.2655650634405429,
      "grad_norm": 25.9968204498291,
      "learning_rate": 1.8678076128651522e-05,
      "loss": 0.2926,
      "step": 900
    },
    {
      "epoch": 0.28031867807612865,
      "grad_norm": 19.07137107849121,
      "learning_rate": 1.8604308055473593e-05,
      "loss": 0.2793,
      "step": 950
    },
    {
      "epoch": 0.29507229271171437,
      "grad_norm": 15.316678047180176,
      "learning_rate": 1.8530539982295664e-05,
      "loss": 0.2806,
      "step": 1000
    },
    {
      "epoch": 0.3098259073473001,
      "grad_norm": 15.96313190460205,
      "learning_rate": 1.8456771909117735e-05,
      "loss": 0.2518,
      "step": 1050
    },
    {
      "epoch": 0.3245795219828858,
      "grad_norm": 14.738668441772461,
      "learning_rate": 1.838300383593981e-05,
      "loss": 0.2685,
      "step": 1100
    },
    {
      "epoch": 0.33933313661847153,
      "grad_norm": 12.907753944396973,
      "learning_rate": 1.830923576276188e-05,
      "loss": 0.2655,
      "step": 1150
    },
    {
      "epoch": 0.35408675125405725,
      "grad_norm": 9.489592552185059,
      "learning_rate": 1.823546768958395e-05,
      "loss": 0.2771,
      "step": 1200
    },
    {
      "epoch": 0.368840365889643,
      "grad_norm": 7.896377086639404,
      "learning_rate": 1.816169961640602e-05,
      "loss": 0.295,
      "step": 1250
    },
    {
      "epoch": 0.3835939805252287,
      "grad_norm": 22.629671096801758,
      "learning_rate": 1.8087931543228092e-05,
      "loss": 0.2774,
      "step": 1300
    },
    {
      "epoch": 0.3983475951608144,
      "grad_norm": 12.766915321350098,
      "learning_rate": 1.8014163470050163e-05,
      "loss": 0.2587,
      "step": 1350
    },
    {
      "epoch": 0.41310120979640014,
      "grad_norm": 18.43453598022461,
      "learning_rate": 1.7940395396872234e-05,
      "loss": 0.2757,
      "step": 1400
    },
    {
      "epoch": 0.42785482443198586,
      "grad_norm": 5.63889741897583,
      "learning_rate": 1.7866627323694305e-05,
      "loss": 0.2521,
      "step": 1450
    },
    {
      "epoch": 0.4426084390675716,
      "grad_norm": 16.67283058166504,
      "learning_rate": 1.779285925051638e-05,
      "loss": 0.2782,
      "step": 1500
    },
    {
      "epoch": 0.4573620537031573,
      "grad_norm": 11.068676948547363,
      "learning_rate": 1.771909117733845e-05,
      "loss": 0.2575,
      "step": 1550
    },
    {
      "epoch": 0.47211566833874297,
      "grad_norm": 13.207789421081543,
      "learning_rate": 1.764532310416052e-05,
      "loss": 0.2662,
      "step": 1600
    },
    {
      "epoch": 0.4868692829743287,
      "grad_norm": 6.417616844177246,
      "learning_rate": 1.757155503098259e-05,
      "loss": 0.2672,
      "step": 1650
    },
    {
      "epoch": 0.5016228976099144,
      "grad_norm": 13.98190975189209,
      "learning_rate": 1.7497786957804662e-05,
      "loss": 0.2607,
      "step": 1700
    },
    {
      "epoch": 0.5163765122455002,
      "grad_norm": 56.860755920410156,
      "learning_rate": 1.7424018884626733e-05,
      "loss": 0.2528,
      "step": 1750
    },
    {
      "epoch": 0.5311301268810859,
      "grad_norm": 6.39507532119751,
      "learning_rate": 1.7350250811448804e-05,
      "loss": 0.2495,
      "step": 1800
    },
    {
      "epoch": 0.5458837415166716,
      "grad_norm": 7.952239036560059,
      "learning_rate": 1.7276482738270878e-05,
      "loss": 0.246,
      "step": 1850
    },
    {
      "epoch": 0.5606373561522573,
      "grad_norm": 8.95653247833252,
      "learning_rate": 1.720271466509295e-05,
      "loss": 0.2585,
      "step": 1900
    },
    {
      "epoch": 0.5753909707878431,
      "grad_norm": 14.418476104736328,
      "learning_rate": 1.712894659191502e-05,
      "loss": 0.2443,
      "step": 1950
    },
    {
      "epoch": 0.5901445854234287,
      "grad_norm": 20.09828758239746,
      "learning_rate": 1.705517851873709e-05,
      "loss": 0.246,
      "step": 2000
    },
    {
      "epoch": 0.6048982000590145,
      "grad_norm": 9.078170776367188,
      "learning_rate": 1.6981410445559165e-05,
      "loss": 0.2356,
      "step": 2050
    },
    {
      "epoch": 0.6196518146946002,
      "grad_norm": 26.492691040039062,
      "learning_rate": 1.6907642372381236e-05,
      "loss": 0.2284,
      "step": 2100
    },
    {
      "epoch": 0.6344054293301858,
      "grad_norm": 18.30030632019043,
      "learning_rate": 1.6833874299203306e-05,
      "loss": 0.2616,
      "step": 2150
    },
    {
      "epoch": 0.6491590439657716,
      "grad_norm": 19.44256019592285,
      "learning_rate": 1.6760106226025377e-05,
      "loss": 0.231,
      "step": 2200
    },
    {
      "epoch": 0.6639126586013573,
      "grad_norm": 11.152538299560547,
      "learning_rate": 1.668633815284745e-05,
      "loss": 0.232,
      "step": 2250
    },
    {
      "epoch": 0.6786662732369431,
      "grad_norm": 8.377274513244629,
      "learning_rate": 1.6612570079669522e-05,
      "loss": 0.2322,
      "step": 2300
    },
    {
      "epoch": 0.6934198878725287,
      "grad_norm": 17.92650032043457,
      "learning_rate": 1.6538802006491593e-05,
      "loss": 0.1981,
      "step": 2350
    },
    {
      "epoch": 0.7081735025081145,
      "grad_norm": 9.584328651428223,
      "learning_rate": 1.6465033933313664e-05,
      "loss": 0.2464,
      "step": 2400
    },
    {
      "epoch": 0.7229271171437002,
      "grad_norm": 16.703493118286133,
      "learning_rate": 1.6391265860135735e-05,
      "loss": 0.2267,
      "step": 2450
    },
    {
      "epoch": 0.737680731779286,
      "grad_norm": 19.0753116607666,
      "learning_rate": 1.6317497786957806e-05,
      "loss": 0.2214,
      "step": 2500
    },
    {
      "epoch": 0.7524343464148716,
      "grad_norm": 15.717314720153809,
      "learning_rate": 1.6243729713779876e-05,
      "loss": 0.2232,
      "step": 2550
    },
    {
      "epoch": 0.7671879610504574,
      "grad_norm": 11.410161018371582,
      "learning_rate": 1.6169961640601947e-05,
      "loss": 0.2258,
      "step": 2600
    },
    {
      "epoch": 0.7819415756860431,
      "grad_norm": 29.092914581298828,
      "learning_rate": 1.609619356742402e-05,
      "loss": 0.2356,
      "step": 2650
    },
    {
      "epoch": 0.7966951903216288,
      "grad_norm": 15.697946548461914,
      "learning_rate": 1.6022425494246092e-05,
      "loss": 0.2629,
      "step": 2700
    },
    {
      "epoch": 0.8114488049572145,
      "grad_norm": 10.823432922363281,
      "learning_rate": 1.5948657421068163e-05,
      "loss": 0.2097,
      "step": 2750
    },
    {
      "epoch": 0.8262024195928003,
      "grad_norm": 9.47752571105957,
      "learning_rate": 1.5874889347890234e-05,
      "loss": 0.2184,
      "step": 2800
    },
    {
      "epoch": 0.8409560342283859,
      "grad_norm": 7.9942946434021,
      "learning_rate": 1.5801121274712305e-05,
      "loss": 0.212,
      "step": 2850
    },
    {
      "epoch": 0.8557096488639717,
      "grad_norm": 15.737624168395996,
      "learning_rate": 1.5727353201534376e-05,
      "loss": 0.2134,
      "step": 2900
    },
    {
      "epoch": 0.8704632634995574,
      "grad_norm": 11.444342613220215,
      "learning_rate": 1.5653585128356446e-05,
      "loss": 0.2385,
      "step": 2950
    },
    {
      "epoch": 0.8852168781351432,
      "grad_norm": 13.559836387634277,
      "learning_rate": 1.557981705517852e-05,
      "loss": 0.2201,
      "step": 3000
    },
    {
      "epoch": 0.8999704927707288,
      "grad_norm": 13.680835723876953,
      "learning_rate": 1.550604898200059e-05,
      "loss": 0.2013,
      "step": 3050
    },
    {
      "epoch": 0.9147241074063146,
      "grad_norm": 10.851712226867676,
      "learning_rate": 1.5432280908822662e-05,
      "loss": 0.2242,
      "step": 3100
    },
    {
      "epoch": 0.9294777220419003,
      "grad_norm": 26.35713768005371,
      "learning_rate": 1.5358512835644733e-05,
      "loss": 0.2174,
      "step": 3150
    },
    {
      "epoch": 0.9442313366774859,
      "grad_norm": 17.868925094604492,
      "learning_rate": 1.5284744762466804e-05,
      "loss": 0.2028,
      "step": 3200
    },
    {
      "epoch": 0.9589849513130717,
      "grad_norm": 10.3679780960083,
      "learning_rate": 1.5210976689288877e-05,
      "loss": 0.1919,
      "step": 3250
    },
    {
      "epoch": 0.9737385659486574,
      "grad_norm": 10.55473804473877,
      "learning_rate": 1.5137208616110947e-05,
      "loss": 0.2068,
      "step": 3300
    },
    {
      "epoch": 0.9884921805842432,
      "grad_norm": 29.585487365722656,
      "learning_rate": 1.5063440542933018e-05,
      "loss": 0.2006,
      "step": 3350
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.9025892593685453,
      "eval_f1": 0.8533999444907022,
      "eval_loss": 0.2210915982723236,
      "eval_precision": 0.9515969299331518,
      "eval_recall": 0.7735735131327363,
      "eval_runtime": 66.6251,
      "eval_samples_per_second": 406.934,
      "eval_steps_per_second": 12.728,
      "step": 3389
    },
    {
      "epoch": 1.0032457952198288,
      "grad_norm": 16.96598243713379,
      "learning_rate": 1.4989672469755092e-05,
      "loss": 0.2058,
      "step": 3400
    },
    {
      "epoch": 1.0179994098554146,
      "grad_norm": 7.5127410888671875,
      "learning_rate": 1.4915904396577163e-05,
      "loss": 0.1857,
      "step": 3450
    },
    {
      "epoch": 1.0327530244910004,
      "grad_norm": 49.05802917480469,
      "learning_rate": 1.4842136323399234e-05,
      "loss": 0.1697,
      "step": 3500
    },
    {
      "epoch": 1.047506639126586,
      "grad_norm": 2.7956058979034424,
      "learning_rate": 1.4768368250221305e-05,
      "loss": 0.1727,
      "step": 3550
    },
    {
      "epoch": 1.0622602537621717,
      "grad_norm": 18.061283111572266,
      "learning_rate": 1.4696075538506935e-05,
      "loss": 0.165,
      "step": 3600
    },
    {
      "epoch": 1.0770138683977575,
      "grad_norm": 20.004423141479492,
      "learning_rate": 1.4622307465329006e-05,
      "loss": 0.1585,
      "step": 3650
    },
    {
      "epoch": 1.0917674830333433,
      "grad_norm": 18.252967834472656,
      "learning_rate": 1.4548539392151077e-05,
      "loss": 0.1755,
      "step": 3700
    },
    {
      "epoch": 1.1065210976689288,
      "grad_norm": 21.52149200439453,
      "learning_rate": 1.4474771318973148e-05,
      "loss": 0.1666,
      "step": 3750
    },
    {
      "epoch": 1.1212747123045146,
      "grad_norm": 22.184127807617188,
      "learning_rate": 1.4401003245795222e-05,
      "loss": 0.1716,
      "step": 3800
    },
    {
      "epoch": 1.1360283269401004,
      "grad_norm": 24.110824584960938,
      "learning_rate": 1.4327235172617293e-05,
      "loss": 0.1546,
      "step": 3850
    },
    {
      "epoch": 1.1507819415756861,
      "grad_norm": 21.988866806030273,
      "learning_rate": 1.4253467099439364e-05,
      "loss": 0.181,
      "step": 3900
    },
    {
      "epoch": 1.1655355562112717,
      "grad_norm": 27.4445858001709,
      "learning_rate": 1.4179699026261435e-05,
      "loss": 0.1511,
      "step": 3950
    },
    {
      "epoch": 1.1802891708468575,
      "grad_norm": 16.927825927734375,
      "learning_rate": 1.4105930953083507e-05,
      "loss": 0.1784,
      "step": 4000
    },
    {
      "epoch": 1.1950427854824432,
      "grad_norm": 19.551342010498047,
      "learning_rate": 1.4032162879905578e-05,
      "loss": 0.1851,
      "step": 4050
    },
    {
      "epoch": 1.2097964001180288,
      "grad_norm": 34.52674102783203,
      "learning_rate": 1.3958394806727649e-05,
      "loss": 0.1622,
      "step": 4100
    },
    {
      "epoch": 1.2245500147536146,
      "grad_norm": 22.00955581665039,
      "learning_rate": 1.388462673354972e-05,
      "loss": 0.143,
      "step": 4150
    },
    {
      "epoch": 1.2393036293892004,
      "grad_norm": 33.73916244506836,
      "learning_rate": 1.3810858660371794e-05,
      "loss": 0.1656,
      "step": 4200
    },
    {
      "epoch": 1.2540572440247861,
      "grad_norm": 16.911300659179688,
      "learning_rate": 1.3737090587193865e-05,
      "loss": 0.1546,
      "step": 4250
    },
    {
      "epoch": 1.268810858660372,
      "grad_norm": 15.922121047973633,
      "learning_rate": 1.3663322514015935e-05,
      "loss": 0.1415,
      "step": 4300
    },
    {
      "epoch": 1.2835644732959575,
      "grad_norm": 23.80780792236328,
      "learning_rate": 1.3589554440838006e-05,
      "loss": 0.1706,
      "step": 4350
    },
    {
      "epoch": 1.2983180879315432,
      "grad_norm": 28.42357635498047,
      "learning_rate": 1.3515786367660077e-05,
      "loss": 0.1613,
      "step": 4400
    },
    {
      "epoch": 1.313071702567129,
      "grad_norm": 31.13276481628418,
      "learning_rate": 1.3442018294482148e-05,
      "loss": 0.1595,
      "step": 4450
    },
    {
      "epoch": 1.3278253172027146,
      "grad_norm": 15.990821838378906,
      "learning_rate": 1.336825022130422e-05,
      "loss": 0.1526,
      "step": 4500
    },
    {
      "epoch": 1.3425789318383003,
      "grad_norm": 15.602317810058594,
      "learning_rate": 1.3294482148126293e-05,
      "loss": 0.1515,
      "step": 4550
    },
    {
      "epoch": 1.3573325464738861,
      "grad_norm": 13.134761810302734,
      "learning_rate": 1.3220714074948364e-05,
      "loss": 0.1455,
      "step": 4600
    },
    {
      "epoch": 1.372086161109472,
      "grad_norm": 25.232847213745117,
      "learning_rate": 1.3146946001770435e-05,
      "loss": 0.1363,
      "step": 4650
    },
    {
      "epoch": 1.3868397757450575,
      "grad_norm": 12.808887481689453,
      "learning_rate": 1.3073177928592507e-05,
      "loss": 0.1382,
      "step": 4700
    },
    {
      "epoch": 1.4015933903806432,
      "grad_norm": 11.383330345153809,
      "learning_rate": 1.2999409855414578e-05,
      "loss": 0.1695,
      "step": 4750
    },
    {
      "epoch": 1.416347005016229,
      "grad_norm": 9.700582504272461,
      "learning_rate": 1.2925641782236649e-05,
      "loss": 0.1458,
      "step": 4800
    },
    {
      "epoch": 1.4311006196518146,
      "grad_norm": 8.072162628173828,
      "learning_rate": 1.285187370905872e-05,
      "loss": 0.1636,
      "step": 4850
    },
    {
      "epoch": 1.4458542342874003,
      "grad_norm": 12.137205123901367,
      "learning_rate": 1.277810563588079e-05,
      "loss": 0.1368,
      "step": 4900
    },
    {
      "epoch": 1.4606078489229861,
      "grad_norm": 35.34477615356445,
      "learning_rate": 1.2704337562702865e-05,
      "loss": 0.1306,
      "step": 4950
    },
    {
      "epoch": 1.475361463558572,
      "grad_norm": 31.35811996459961,
      "learning_rate": 1.2630569489524936e-05,
      "loss": 0.157,
      "step": 5000
    },
    {
      "epoch": 1.4901150781941577,
      "grad_norm": 21.140073776245117,
      "learning_rate": 1.2556801416347006e-05,
      "loss": 0.1458,
      "step": 5050
    },
    {
      "epoch": 1.5048686928297434,
      "grad_norm": 17.11158561706543,
      "learning_rate": 1.2483033343169077e-05,
      "loss": 0.1466,
      "step": 5100
    },
    {
      "epoch": 1.519622307465329,
      "grad_norm": 28.526233673095703,
      "learning_rate": 1.2409265269991148e-05,
      "loss": 0.1506,
      "step": 5150
    },
    {
      "epoch": 1.5343759221009146,
      "grad_norm": 13.471609115600586,
      "learning_rate": 1.233549719681322e-05,
      "loss": 0.1509,
      "step": 5200
    },
    {
      "epoch": 1.5491295367365003,
      "grad_norm": 20.89297866821289,
      "learning_rate": 1.2261729123635291e-05,
      "loss": 0.1299,
      "step": 5250
    },
    {
      "epoch": 1.5638831513720861,
      "grad_norm": 21.084692001342773,
      "learning_rate": 1.2187961050457362e-05,
      "loss": 0.1305,
      "step": 5300
    },
    {
      "epoch": 1.5786367660076719,
      "grad_norm": 32.804107666015625,
      "learning_rate": 1.2114192977279435e-05,
      "loss": 0.1551,
      "step": 5350
    },
    {
      "epoch": 1.5933903806432577,
      "grad_norm": 11.768656730651855,
      "learning_rate": 1.2040424904101507e-05,
      "loss": 0.153,
      "step": 5400
    },
    {
      "epoch": 1.6081439952788434,
      "grad_norm": 15.667191505432129,
      "learning_rate": 1.1966656830923578e-05,
      "loss": 0.1319,
      "step": 5450
    },
    {
      "epoch": 1.622897609914429,
      "grad_norm": 11.405047416687012,
      "learning_rate": 1.1892888757745649e-05,
      "loss": 0.149,
      "step": 5500
    },
    {
      "epoch": 1.6376512245500148,
      "grad_norm": 14.968426704406738,
      "learning_rate": 1.181912068456772e-05,
      "loss": 0.1573,
      "step": 5550
    },
    {
      "epoch": 1.6524048391856003,
      "grad_norm": 9.268739700317383,
      "learning_rate": 1.174535261138979e-05,
      "loss": 0.1123,
      "step": 5600
    },
    {
      "epoch": 1.667158453821186,
      "grad_norm": 0.7226022481918335,
      "learning_rate": 1.1671584538211861e-05,
      "loss": 0.1155,
      "step": 5650
    },
    {
      "epoch": 1.6819120684567719,
      "grad_norm": 25.936809539794922,
      "learning_rate": 1.1597816465033936e-05,
      "loss": 0.1514,
      "step": 5700
    },
    {
      "epoch": 1.6966656830923577,
      "grad_norm": 4.437021255493164,
      "learning_rate": 1.1525523753319564e-05,
      "loss": 0.1342,
      "step": 5750
    },
    {
      "epoch": 1.7114192977279434,
      "grad_norm": 7.340378761291504,
      "learning_rate": 1.1451755680141637e-05,
      "loss": 0.1281,
      "step": 5800
    },
    {
      "epoch": 1.7261729123635292,
      "grad_norm": 38.44197463989258,
      "learning_rate": 1.1377987606963708e-05,
      "loss": 0.1385,
      "step": 5850
    },
    {
      "epoch": 1.7409265269991148,
      "grad_norm": 19.646854400634766,
      "learning_rate": 1.1304219533785779e-05,
      "loss": 0.1441,
      "step": 5900
    },
    {
      "epoch": 1.7556801416347005,
      "grad_norm": 18.771682739257812,
      "learning_rate": 1.123045146060785e-05,
      "loss": 0.1074,
      "step": 5950
    },
    {
      "epoch": 1.770433756270286,
      "grad_norm": 68.22604370117188,
      "learning_rate": 1.115668338742992e-05,
      "loss": 0.1493,
      "step": 6000
    },
    {
      "epoch": 1.7851873709058719,
      "grad_norm": 18.19668197631836,
      "learning_rate": 1.1082915314251991e-05,
      "loss": 0.1472,
      "step": 6050
    },
    {
      "epoch": 1.7999409855414576,
      "grad_norm": 19.421070098876953,
      "learning_rate": 1.1009147241074064e-05,
      "loss": 0.1262,
      "step": 6100
    },
    {
      "epoch": 1.8146946001770434,
      "grad_norm": 14.443294525146484,
      "learning_rate": 1.0935379167896136e-05,
      "loss": 0.124,
      "step": 6150
    },
    {
      "epoch": 1.8294482148126292,
      "grad_norm": 23.4581356048584,
      "learning_rate": 1.0861611094718207e-05,
      "loss": 0.1235,
      "step": 6200
    },
    {
      "epoch": 1.8442018294482148,
      "grad_norm": 26.076244354248047,
      "learning_rate": 1.0787843021540278e-05,
      "loss": 0.1312,
      "step": 6250
    },
    {
      "epoch": 1.8589554440838005,
      "grad_norm": 24.008359909057617,
      "learning_rate": 1.071407494836235e-05,
      "loss": 0.1366,
      "step": 6300
    },
    {
      "epoch": 1.873709058719386,
      "grad_norm": 18.45757484436035,
      "learning_rate": 1.0640306875184421e-05,
      "loss": 0.1125,
      "step": 6350
    },
    {
      "epoch": 1.8884626733549719,
      "grad_norm": 15.518179893493652,
      "learning_rate": 1.0566538802006492e-05,
      "loss": 0.1521,
      "step": 6400
    },
    {
      "epoch": 1.9032162879905576,
      "grad_norm": 27.364089965820312,
      "learning_rate": 1.0492770728828563e-05,
      "loss": 0.1245,
      "step": 6450
    },
    {
      "epoch": 1.9179699026261434,
      "grad_norm": 8.647560119628906,
      "learning_rate": 1.0419002655650637e-05,
      "loss": 0.1092,
      "step": 6500
    },
    {
      "epoch": 1.9327235172617292,
      "grad_norm": 19.441158294677734,
      "learning_rate": 1.0345234582472708e-05,
      "loss": 0.1248,
      "step": 6550
    },
    {
      "epoch": 1.947477131897315,
      "grad_norm": 7.930138111114502,
      "learning_rate": 1.0271466509294779e-05,
      "loss": 0.1161,
      "step": 6600
    },
    {
      "epoch": 1.9622307465329005,
      "grad_norm": 27.12995719909668,
      "learning_rate": 1.019769843611685e-05,
      "loss": 0.1258,
      "step": 6650
    },
    {
      "epoch": 1.9769843611684863,
      "grad_norm": 36.4705696105957,
      "learning_rate": 1.012393036293892e-05,
      "loss": 0.1145,
      "step": 6700
    },
    {
      "epoch": 1.9917379758040719,
      "grad_norm": 59.33979034423828,
      "learning_rate": 1.0050162289760991e-05,
      "loss": 0.1151,
      "step": 6750
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.9505385069341988,
      "eval_f1": 0.9309581424084847,
      "eval_loss": 0.14828503131866455,
      "eval_precision": 0.9530887623866751,
      "eval_recall": 0.9098319412297474,
      "eval_runtime": 66.6645,
      "eval_samples_per_second": 406.693,
      "eval_steps_per_second": 12.72,
      "step": 6778
    }
  ],
  "logging_steps": 50,
  "max_steps": 13556,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 4,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1295802873630720.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
